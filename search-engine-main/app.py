from math import ceil
from flask import Flask, render_template, request, url_for
from markupsafe import escape
from whoosh.index import open_dir
from whoosh.qparser import MultifieldParser, PhrasePlugin
from whoosh.scoring import BM25F
from config import INDEX_DIR
from bs4 import BeautifulSoup
import re


app = Flask(__name__)

def extract_main_content(html):
    soup = BeautifulSoup(html, 'html.parser')

    selectors = [
        {"id": "main"},
        {"role": "main"},
        {"class_": "article-body"},
        {"class_": "story-body"},
        {"class_": "main-content"},
        {"class_": "article__content"},
    ]
    for sel in selectors:
        main = soup.find(**sel)
        if main:
            return main.get_text(separator=' ', strip=True)

    # fallback: article tag
    article = soup.find('article')
    if article:
        return article.get_text(separator=' ', strip=True)

    # fallback: Ø£ÙˆÙ„ 3 ÙÙ‚Ø±Ø§Øª
    paragraphs = soup.find_all('p')
    long_paras = [p.get_text() for p in paragraphs if len(p.get_text()) > 100]
    if long_paras:
        return ' '.join(long_paras[:3])

    return soup.get_text(separator=' ', strip=True)


# ğŸ” Ø¯Ø§Ù„Ø© ØªÙ„Ø®ÙŠØµ Ø°ÙƒÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„Ø±ÙŠØ§Ø¶Ø§Øª
def summarize_result(content, query):
    content_lower = content.lower()

    # Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    if "live" in content_lower or "results" in content_lower or "score" in content_lower:
        kind = "Live Scores Page"
    elif "schedule" in content_lower or "fixtures" in content_lower:
        kind = "Match Schedule"
    elif "news" in content_lower or "preview" in content_lower:
        kind = "News Article"
    else:
        kind = "General Sports Content"

    # Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø±ÙŠØ§Ø¶Ø§Øª
    sports_keywords = ["football", "soccer", "tennis", "golf", "basketball", "f1", "cricket", "rugby", "nfl"]
    sports_found = [sport.capitalize() for sport in sports_keywords if sport in content_lower]
    sports_str = ", ".join(sports_found) if sports_found else "Unspecified"

    # Ø­Ø§Ù„Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ«
    is_live = "Live" if any(word in content_lower for word in ["live", "now", "updated", "streaming"]) else "Not Live"

    return kind, sports_str, is_live

def extract_weighted_keywords_snippet(text, query, max_words=20):
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
    words = re.findall(r'\b\w+\b', text.lower())
    query_words = query.lower().split()

    # Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø­ÙŠØ·Ø© Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
    important_words = []
    for i, word in enumerate(words):
        if any(q in word for q in query_words):
            # Ù†Ø£Ø®Ø° Ø§Ù„Ø¬ÙŠØ±Ø§Ù†: 3 Ù‚Ø¨Ù„ Ùˆ3 Ø¨Ø¹Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø©
            context = words[max(0, i-3): i+4]
            important_words.extend(context)

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø¥Ø¶Ø§ÙØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
    stop_words = {"the", "and", "a", "to", "of", "in", "on", "for", "is", "are", "be", "by", "with", "at", "from", "it"}
    filtered = [w for w in important_words if w not in stop_words]

    # Ù†Ø®ØªØ§Ø± Ø£ÙˆÙ„ 20 ÙƒÙ„Ù…Ø© ÙÙ‚Ø·
    snippet = " ".join(filtered[:max_words])
    return snippet.capitalize() + "..." if snippet else text[:200] + "..."


# ØµÙØ­Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
@app.route("/")
def home():
    return render_template("search.html", search_url=url_for("results"))

# ØµÙØ­Ø© Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
@app.route("/results")
def results():
    query = request.args.get("q", "")
    page = request.args.get("page", 1, type=int)
    per_page = 5

    if not query.strip():
        return render_template(
            "results.html",
            query=query,
            results_list=[],
            search_url=url_for("results"),
            pagination=None,
        )

    ix = open_dir(INDEX_DIR)
    qp = MultifieldParser(["title", "content"], schema=ix.schema)
    qp.add_plugin(PhrasePlugin())
    q = qp.parse(escape(query))

    with ix.searcher(weighting=BM25F(B=0.5, K1=1.5)) as searcher:
        results = searcher.search(q, limit=None)
        total_results = len(results)
        total_pages = ceil(total_results / per_page)

        start = (page - 1) * per_page
        end = start + per_page
        current_results = results[start:end]

        results_list = []
        for result in current_results:
            raw_html = result.get("content", "")
            main_text = extract_main_content(raw_html)
            teaser = extract_weighted_keywords_snippet(main_text, query)
            kind, sports_str, is_live = summarize_result(main_text, query)

            results_list.append({
                "url": result["url"],
                "title": result["title"],
                "teaser": teaser,
                "author": result.get("author", "Unknown"),
                "trust_score": result.get("trust_score", 0),
                "kind": kind,
                "sports": sports_str,
                "live_status": is_live
            })

        pagination = {
            "page": page,
            "per_page": per_page,
            "total_results": total_results,
            "total_pages": total_pages,
            "has_prev": page > 1,
            "has_next": page < total_pages,
            "prev_num": page - 1,
            "next_num": page + 1,
        }

    return render_template(
        "results.html",
        query=query,
        results_list=results_list,
        search_url=url_for("results"),
        pagination=pagination,
    )
